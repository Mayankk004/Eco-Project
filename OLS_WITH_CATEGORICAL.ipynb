{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e571529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    FunctionTransformer,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b6063",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3695821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of dataframe is 246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_214768/1993380002.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(axis =1 ,columns=['statename'],inplace=True)\n",
      "/tmp/ipykernel_214768/1993380002.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"unirrigatedarea1000hecatres\"] = (\n",
      "/tmp/ipykernel_214768/1993380002.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"irrigatedarea1000hectares\"] = df[\"irrigatedarea1000hectares\"] + 1\n",
      "/tmp/ipykernel_214768/1993380002.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"salinity_alkalinity_percent\"] = df[\"salinity_alkalinity_percent\"] + 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Common  columns : Crop,Year\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "\n",
    "    df.drop(\n",
    "        axis=1,\n",
    "        columns=[\n",
    "            \"districtcode\",\n",
    "            \"statecode\",\n",
    "            \"year\",\n",
    "            \"crop\",\n",
    "            \"districtname\",\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    region_map = {\n",
    "        \"Chandigarh\": \"North\", \"Delhi\": \"North\", \"Haryana\": \"North\", \"Himachal Pradesh\": \"North\",\n",
    "        \"Jammu & Kashmir\": \"North\", \"Punjab\": \"North\", \"Rajasthan\": \"North\",\n",
    "        \"Arunachal Pradesh\": \"North-East\", \"Assam\": \"North-East\", \"Manipur\": \"North-East\",\n",
    "        \"Meghalaya\": \"North-East\", \"Mizoram\": \"North-East\", \"Nagaland\": \"North-East\", \"Tripura\": \"North-East\",\n",
    "        \"Andaman & Nicobar Islands\": \"East\", \"Bihar\": \"East\", \"Jharkhand\": \"East\",\n",
    "        \"Odisha\": \"East\", \"Sikkim\": \"East\", \"West Bengal\": \"East\",\n",
    "        \"Chhattisgarh\": \"Central\", \"Madhya Pradesh\": \"Central\", \"Uttar Pradesh\": \"Central\", \"Uttarakhand\": \"Central\",\n",
    "        \"Dadra & Nagar Haveli\": \"West\", \"Daman & Diu\": \"West\", \"Goa\": \"West\",\n",
    "        \"Gujarat\": \"West\", \"Maharashtra\": \"West\",\n",
    "        \"Andhra Pradesh\": \"South\", \"Karnataka\": \"South\", \"Kerala\": \"South\",\n",
    "        \"Lakshadweep\": \"South\", \"Puducherry\": \"South\", \"Tamil Nadu\": \"South\"\n",
    "    }\n",
    "\n",
    "    # Drop rows with unmatched states\n",
    "    \n",
    "    df[\"region\"] = df[\"statename\"].map(region_map)\n",
    "    \n",
    "    df = df.dropna(subset=[\"region\"])\n",
    "    \n",
    "    df.drop(axis =1 ,columns=['statename'],inplace=True)\n",
    "    print(f\"lenght of dataframe is {len(df)}\")\n",
    "\n",
    "    categorical_features = [\"region\"]\n",
    "\n",
    "    cobb_douglas_features = [\n",
    "        \"area1000hectares\",\n",
    "        \"irrigatedarea1000hectares\",\n",
    "        \"unirrigatedarea1000hecatres\",\n",
    "        \"nitrogenconsumptiontonnes\",\n",
    "        \"phosphateconsumptiontonnes\",\n",
    "        \"potashconsumptiontonnes\",\n",
    "        \"production1000tonnes\",\n",
    "    ]\n",
    "\n",
    "    env_features = [\n",
    "        \"total_rainfall\",\n",
    "        \"average_rainfall\",\n",
    "        \"salinity_alkalinity_percent\",\n",
    "    ]\n",
    "\n",
    "    df[\"unirrigatedarea1000hecatres\"] = (\n",
    "        df[\"area1000hectares\"] - df[\"irrigatedarea1000hectares\"] + 1\n",
    "    )\n",
    "\n",
    "    df[\"irrigatedarea1000hectares\"] = df[\"irrigatedarea1000hectares\"] + 1\n",
    "\n",
    "    df[\"salinity_alkalinity_percent\"] = df[\"salinity_alkalinity_percent\"] + 1\n",
    "\n",
    "    log_transform = Pipeline(\n",
    "        steps=[\n",
    "            (\"log\", FunctionTransformer(np.log, validate=True)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[(\"onehot\", OneHotEncoder( handle_unknown=\"ignore\"))]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", log_transform, cobb_douglas_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "            (\"passthrough\", \"passthrough\", env_features)\n",
    "        ],sparse_threshold=0.0,remainder=\"drop\"\n",
    "    )\n",
    "    x_transform = preprocessor.fit_transform(df)\n",
    "    cat_column_names = preprocessor.named_transformers_[\"cat\"][\"onehot\"].get_feature_names_out(categorical_features)\n",
    "    X_df = pd.DataFrame(x_transform, columns=cobb_douglas_features + env_features + list(cat_column_names))\n",
    "\n",
    "    output_file = \"output.txt\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for i in X_df.iloc[0].items():\n",
    "\n",
    "            f.write(str(i))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    X = X_df.drop(axis=1, columns=[\"production1000tonnes\"])\n",
    "    # print(X.describe())\n",
    "    Y = X_df[\"production1000tonnes\"]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "path_to_dataset = \"Dataset/Task2Summary.csv\"\n",
    "df = pd.read_csv(path_to_dataset)\n",
    "\n",
    "\n",
    "X_copy, Y_Copy = preprocess_data(df)\n",
    "X, Y = torch.tensor(X_copy.to_numpy()), torch.tensor(Y_Copy.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b4b85a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OLS(nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.num_features = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.num_features == None:\n",
    "            raise LookupError(\"Train the OLS First :)\")\n",
    "\n",
    "        # X = (X - X.mean(dim=0, keepdim=True)) / X.std(dim=0, keepdim=True)\n",
    "        return torch.matmul(X, self.weights) + self.bias\n",
    "\n",
    "    def calculate_statistics(self, y_prime: torch.tensor, y: torch.tensor):\n",
    "        SST = ((y - y.mean()) ** 2).sum()\n",
    "\n",
    "        SSR = ((y_prime - y.mean()) ** 2).sum()\n",
    "\n",
    "        SSE = ((y_prime - y) ** 2).sum()\n",
    "\n",
    "        EXPECTEDVALUE_U = (y - y_prime).sum()\n",
    "\n",
    "        R2 = 1 - (SSE / SST)\n",
    "\n",
    "        n = y.shape[0]\n",
    "        p = self.num_features  \n",
    "        adj_R2 = 1 - ((1 - R2) * (n - 1)) / (n - p - 1)\n",
    "\n",
    "        print(\n",
    "            f\"Sum of squared Terms {SST.item():.4f}\\nSum of squared errors {SSE.item():.4f} \\nRegression sum of squares {SSR.item():.4f} \\nExpected value of U {EXPECTEDVALUE_U.item():.4f} \\nR2 {R2} \\nadjustedR2 {adj_R2}\"\n",
    "        )\n",
    "        return SSR, SSE, SST\n",
    "\n",
    "    def fit(self, dataset):\n",
    "\n",
    "        # Only accepts full batch\n",
    "        X,Y = dataset\n",
    "        Y = Y.unsqueeze(1)\n",
    "        n, p = X.shape\n",
    "        X_aug = torch.cat([torch.ones(n,1), X], dim=1)   # (n, p+1)\n",
    "        # closed‑form\n",
    "        w_full = torch.linalg.pinv(X_aug) @ Y            # (p+1,1)\n",
    "\n",
    "        self.bias    = nn.Parameter(w_full[0:1, :])       # (1,1)\n",
    "        self.weights = nn.Parameter(w_full[1:, :])       # (p,1)\n",
    "        self.num_features = p\n",
    "        print(f\"Trained OLS for {self.num_features} features\")\n",
    "        print(f\"Beta matrix is {self.weights} (without beta not) ,Beta not = { self.bias }\")\n",
    "        print('\\n')\n",
    "        self.calculate_statistics(self(X), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0680c4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained OLS for 15 features\n",
      "Beta matrix is Parameter containing:\n",
      "tensor([[ 9.7237e-01],\n",
      "        [ 2.2199e-02],\n",
      "        [-1.8103e-01],\n",
      "        [-1.7440e-02],\n",
      "        [ 8.8580e-02],\n",
      "        [-1.1508e-02],\n",
      "        [-9.1308e-03],\n",
      "        [ 1.4772e-01],\n",
      "        [ 2.8890e-01],\n",
      "        [-2.3385e-01],\n",
      "        [-2.6597e-01],\n",
      "        [-5.7998e-01],\n",
      "        [-2.0176e-04],\n",
      "        [ 7.9323e-03],\n",
      "        [ 2.4529e-03]], dtype=torch.float64, requires_grad=True) (without beta not) ,Beta not = Parameter containing:\n",
      "tensor([[-0.6523]], dtype=torch.float64, requires_grad=True)\n",
      "\n",
      "\n",
      "Sum of squared Terms 621.9362\n",
      "Sum of squared errors 65.2809 \n",
      "Regression sum of squares 556.6553 \n",
      "Expected value of U 0.0000 \n",
      "R2 0.8950360178866549 \n",
      "adjustedR2 0.8881905407923063\n"
     ]
    }
   ],
   "source": [
    "model = OLS()\n",
    "model.fit((X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1c40d",
   "metadata": {},
   "source": [
    "Useful for later(q8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "344c3b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSR_r = torch.sum((Y - model(X)) ** 2).item()  # base model\n",
    "k_r = model.weights.shape[0]# base model parameters\n",
    "n = X.shape[0]\n",
    "df_r = n - k_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cfcc734",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Task2Summary.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This is for EDA\u001b[39;00m\n\u001b[1;32m      3\u001b[0m FILE_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask2Summary.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFILE_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset Info:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39minfo())\n",
      "File \u001b[0;32m~/anaconda3/envs/katana/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/katana/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/katana/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/katana/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/katana/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Task2Summary.csv'"
     ]
    }
   ],
   "source": [
    "# This is for EDA\n",
    "\n",
    "FILE_NAME = \"Task2Summary.csv\"\n",
    "\n",
    "df = pd.read_csv(FILE_NAME)\n",
    "\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"Duplicate Rows:\", df.duplicated().sum())\n",
    "\n",
    "if \"target\" in df.columns:\n",
    "    print(\"Target Distribution:\")\n",
    "    print(df[\"target\"].value_counts())\n",
    "    sns.countplot(data=df, x=\"target\")\n",
    "    plt.title(\"Target Class Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "df.hist(figsize=(12, 8), bins=30)\n",
    "plt.suptitle(\"Histogram of Features\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.boxplot(data=df, x=col)\n",
    "    plt.title(f\"Boxplot: {col}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# cat_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "# for col in cat_cols:\n",
    "#     print(f\"Value Counts for {col}:\")\n",
    "#     print(df[col].value_counts())\n",
    "#     sns.countplot(data=df, x=col)\n",
    "#     plt.title(f\"Distribution of {col}\")\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b9b83f",
   "metadata": {},
   "source": [
    "✅ Question 6: Diminishing Marginal Product (Quadratic Model)\n",
    "From the quadratic model regression:\n",
    "\n",
    "Squared terms like area1000hectares_sq, irrigatedarea1000hectares_sq, nitrogenconsumptiontonnes_sq, etc., mostly have negative coefficients.\n",
    "These negative signs indicate diminishing returns—as input increases, additional output increases at a decreasing rate.\n",
    "Many of these squared terms are statistically significant (p-value < 0.05), confirming that:\n",
    "✅ We reject the null hypothesis for those inputs:\n",
    "\n",
    "\"Marginal productivity diminishes as input levels rise beyond a threshold.\"\n",
    "✅ Question 7: Complementarity of Irrigation and Fertilizer\n",
    "From the interaction model:\n",
    "\n",
    "The coefficient of the interaction term irrigation_x_nitrogen is:\n",
    "Positive, suggesting that increased irrigation amplifies the effect of nitrogen application.\n",
    "Statistically significant (p-value < 0.05), providing evidence for input complementarity.\n",
    "✅ We reject the null hypothesis:\n",
    "\n",
    "\"There is a positive interaction between irrigation and nitrogen — they are complementary inputs.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab1745",
   "metadata": {},
   "source": [
    "/Users/dhruvyadav/Downloads/Task2Summary.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fef4bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bef81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Dataset/Task2Summary.csv\")\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df.drop(\n",
    "    axis=1,\n",
    "    columns=[\"districtcode\", \"statename\", \"statecode\", \"year\", \"crop\", \"districtname\"],\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Base input features\n",
    "base_features = [\n",
    "    \"area1000hectares\", \"production1000tonnes\", \"irrigatedarea1000hectares\",\n",
    "    \"nitrogenconsumptiontonnes\", \"phosphateconsumptiontonnes\", \"potashconsumptiontonnes\",\n",
    "    \"total_rainfall\", \"average_rainfall\", \"salinity_alkalinity_percent\"\n",
    "]\n",
    "\n",
    "# Add unirrigated area\n",
    "df[\"unirrigatedarea1000hecatres\"] = df[\"area1000hectares\"] - df[\"irrigatedarea1000hectares\"]\n",
    "\n",
    "# Add squared terms for quadratic model\n",
    "for feature in base_features:\n",
    "    if feature != \"production1000tonnes\":\n",
    "        df[f\"{feature}_squared\"] = df[feature] ** 2\n",
    "\n",
    "# Define inputs and output\n",
    "X = df.drop(columns=[\"production1000tonnes\"])\n",
    "Y = df[\"production1000tonnes\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d593a7a5",
   "metadata": {},
   "source": [
    "This is the model to test for diminishing marginal product of inputs by checking if squared coefficients are negative and significant.\n",
    "For each input, if its squared term has:\n",
    "Negative coefficient ✅\n",
    "p-value < 0.05 ✅\n",
    "→ Then it shows diminishing marginal returns for that input.\n",
    "Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de73243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             OLS Regression Results                             \n",
      "================================================================================\n",
      "Dep. Variable:     production1000tonnes   R-squared:                       0.878\n",
      "Model:                              OLS   Adj. R-squared:                  0.870\n",
      "Method:                   Least Squares   F-statistic:                     106.8\n",
      "Date:                  Tue, 22 Apr 2025   Prob (F-statistic):           8.00e-99\n",
      "Time:                          13:33:57   Log-Likelihood:                -913.49\n",
      "No. Observations:                   255   AIC:                             1861.\n",
      "Df Residuals:                       238   BIC:                             1921.\n",
      "Df Model:                            16                                         \n",
      "Covariance Type:              nonrobust                                         \n",
      "=======================================================================================================\n",
      "                                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "const                                 -13.7664      3.788     -3.634      0.000     -21.229      -6.304\n",
      "area1000hectares                        0.6435      0.248      2.592      0.010       0.154       1.133\n",
      "irrigatedarea1000hectares               0.6491      0.498      1.303      0.194      -0.332       1.630\n",
      "nitrogenconsumptiontonnes              -0.0001   9.18e-05     -1.112      0.267      -0.000    7.87e-05\n",
      "phosphateconsumptiontonnes              0.0005      0.000      2.240      0.026     6.4e-05       0.001\n",
      "potashconsumptiontonnes                -0.0007      0.000     -2.097      0.037      -0.001   -4.44e-05\n",
      "total_rainfall                         -0.0155      0.024     -0.658      0.511      -0.062       0.031\n",
      "average_rainfall                        0.6090      0.350      1.741      0.083      -0.080       1.298\n",
      "salinity_alkalinity_percent            -0.1325      0.170     -0.778      0.437      -0.468       0.203\n",
      "unirrigatedarea1000hecatres            -0.0056      0.253     -0.022      0.982      -0.504       0.493\n",
      "area1000hectares_squared               -0.0011      0.000     -2.791      0.006      -0.002      -0.000\n",
      "irrigatedarea1000hectares_squared       0.0045      0.075      0.060      0.952      -0.142       0.151\n",
      "nitrogenconsumptiontonnes_squared    1.196e-09   4.88e-10      2.453      0.015    2.35e-10    2.16e-09\n",
      "phosphateconsumptiontonnes_squared   -1.03e-08    3.2e-09     -3.222      0.001   -1.66e-08      -4e-09\n",
      "potashconsumptiontonnes_squared      1.718e-08   1.31e-08      1.311      0.191   -8.64e-09     4.3e-08\n",
      "total_rainfall_squared               1.157e-05    1.7e-05      0.681      0.497   -2.19e-05     4.5e-05\n",
      "average_rainfall_squared               -0.0043      0.003     -1.459      0.146      -0.010       0.001\n",
      "salinity_alkalinity_percent_squared     0.0002      0.004      0.040      0.968      -0.008       0.009\n",
      "==============================================================================\n",
      "Omnibus:                       37.984   Durbin-Watson:                   1.700\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              189.996\n",
      "Skew:                          -0.414   Prob(JB):                     5.53e-42\n",
      "Kurtosis:                       7.147   Cond. No.                     6.14e+18\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 3.97e-16. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Add constant/intercept\n",
    "X_quad_const = sm.add_constant(X)\n",
    "\n",
    "# Fit the quadratic OLS model\n",
    "model_quad = sm.OLS(Y, X_quad_const).fit()\n",
    "\n",
    "# Display results\n",
    "print(model_quad.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e184afa",
   "metadata": {},
   "source": [
    "If irrigation_x_nitrogen coefficient is:\n",
    "Positive ✅\n",
    "Statistically significant (p < 0.05) ✅\n",
    "→ Then there's positive complementarity between irrigation and nitrogen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba51892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             OLS Regression Results                             \n",
      "================================================================================\n",
      "Dep. Variable:     production1000tonnes   R-squared:                       0.879\n",
      "Model:                              OLS   Adj. R-squared:                  0.871\n",
      "Method:                   Least Squares   F-statistic:                     101.7\n",
      "Date:                  Tue, 22 Apr 2025   Prob (F-statistic):           1.63e-98\n",
      "Time:                          13:34:12   Log-Likelihood:                -911.71\n",
      "No. Observations:                   255   AIC:                             1859.\n",
      "Df Residuals:                       237   BIC:                             1923.\n",
      "Df Model:                            17                                         \n",
      "Covariance Type:              nonrobust                                         \n",
      "=======================================================================================================\n",
      "                                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "const                                 -12.7130      3.814     -3.334      0.001     -20.226      -5.200\n",
      "area1000hectares                        0.5052      0.258      1.955      0.052      -0.004       1.014\n",
      "irrigatedarea1000hectares               0.3606      0.520      0.693      0.489      -0.664       1.386\n",
      "nitrogenconsumptiontonnes              -0.0001   9.25e-05     -1.382      0.168      -0.000    5.44e-05\n",
      "phosphateconsumptiontonnes              0.0006      0.000      2.553      0.011       0.000       0.001\n",
      "potashconsumptiontonnes                -0.0008      0.000     -2.343      0.020      -0.002      -0.000\n",
      "total_rainfall                         -0.0174      0.023     -0.741      0.459      -0.064       0.029\n",
      "average_rainfall                        0.6213      0.348      1.785      0.076      -0.065       1.307\n",
      "salinity_alkalinity_percent            -0.1753      0.171     -1.025      0.306      -0.512       0.162\n",
      "unirrigatedarea1000hecatres             0.1445      0.265      0.546      0.586      -0.377       0.666\n",
      "area1000hectares_squared               -0.0011      0.000     -2.943      0.004      -0.002      -0.000\n",
      "irrigatedarea1000hectares_squared      -0.0037      0.074     -0.049      0.961      -0.150       0.143\n",
      "nitrogenconsumptiontonnes_squared    1.241e-09   4.86e-10      2.554      0.011    2.84e-10     2.2e-09\n",
      "phosphateconsumptiontonnes_squared  -1.187e-08    3.3e-09     -3.603      0.000   -1.84e-08   -5.38e-09\n",
      "potashconsumptiontonnes_squared      2.075e-08   1.32e-08      1.573      0.117   -5.23e-09    4.67e-08\n",
      "total_rainfall_squared               1.288e-05   1.69e-05      0.761      0.447   -2.05e-05    4.62e-05\n",
      "average_rainfall_squared               -0.0044      0.003     -1.514      0.131      -0.010       0.001\n",
      "salinity_alkalinity_percent_squared     0.0011      0.004      0.246      0.806      -0.008       0.010\n",
      "irrigation_x_nitrogen                8.622e-06   4.73e-06      1.824      0.069   -6.89e-07    1.79e-05\n",
      "==============================================================================\n",
      "Omnibus:                       37.794   Durbin-Watson:                   1.700\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              190.433\n",
      "Skew:                          -0.406   Prob(JB):                     4.45e-42\n",
      "Kurtosis:                       7.155   Cond. No.                     3.02e+18\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.64e-15. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "# Add interaction term: irrigation × nitrogen\n",
    "X[\"irrigation_x_nitrogen\"] = X[\"irrigatedarea1000hectares\"] * X[\"nitrogenconsumptiontonnes\"]\n",
    "\n",
    "# Fit OLS model with interaction term\n",
    "X_interact_const = sm.add_constant(X)\n",
    "model_interact = sm.OLS(Y, X_interact_const).fit()\n",
    "\n",
    "# Display results\n",
    "print(model_interact.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f537e",
   "metadata": {},
   "source": [
    "If region dummy coefficients are significant → there are statistical differences across regions.\n",
    "Question 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998ddc2",
   "metadata": {},
   "source": [
    "Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6433a0a1",
   "metadata": {},
   "source": [
    "Model-A (Enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ec7c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "Trained OLS for 44 features\n",
      "Sum of squared Terms 621.9362\n",
      "Sum of squared errors 294.0233 \n",
      "Regression sum of squares 831.9655 \n",
      "Expected value of U 36.3207 \n",
      "R2 0.527245283126831 \n",
      "adjustedR2 0.4237567186355591\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Common  columns : Crop,Year\n",
    "def preprocess_data(df):\n",
    "    df = df.copy()  # Always work on a fresh copy to avoid warnings\n",
    "\n",
    "    # Adjust irrigated area to handle log(0)\n",
    "    df[\"irrigatedarea1000hectares\"] += 1\n",
    "    df[\"unirrigatedarea1000hecatres\"] = df[\"area1000hectares\"] - df[\"irrigatedarea1000hectares\"]\n",
    "\n",
    "    # Add region column\n",
    "    region_map = {\n",
    "        \"Chandigarh\": \"North\", \"Delhi\": \"North\", \"Haryana\": \"North\", \"Himachal Pradesh\": \"North\",\n",
    "        \"Jammu & Kashmir\": \"North\", \"Punjab\": \"North\", \"Rajasthan\": \"North\",\n",
    "        \"Arunachal Pradesh\": \"North-East\", \"Assam\": \"North-East\", \"Manipur\": \"North-East\",\n",
    "        \"Meghalaya\": \"North-East\", \"Mizoram\": \"North-East\", \"Nagaland\": \"North-East\", \"Tripura\": \"North-East\",\n",
    "        \"Andaman & Nicobar Islands\": \"East\", \"Bihar\": \"East\", \"Jharkhand\": \"East\",\n",
    "        \"Odisha\": \"East\", \"Sikkim\": \"East\", \"West Bengal\": \"East\",\n",
    "        \"Chhattisgarh\": \"Central\", \"Madhya Pradesh\": \"Central\", \"Uttar Pradesh\": \"Central\", \"Uttarakhand\": \"Central\",\n",
    "        \"Dadra & Nagar Haveli\": \"West\", \"Daman & Diu\": \"West\", \"Goa\": \"West\",\n",
    "        \"Gujarat\": \"West\", \"Maharashtra\": \"West\",\n",
    "        \"Andhra Pradesh\": \"South\", \"Karnataka\": \"South\", \"Kerala\": \"South\",\n",
    "        \"Lakshadweep\": \"South\", \"Puducherry\": \"South\", \"Tamil Nadu\": \"South\"\n",
    "    }\n",
    "    df[\"region\"] = df[\"statename\"].map(region_map)\n",
    "\n",
    "    # Drop rows with unmatched states\n",
    "    df = df.dropna(subset=[\"region\"])\n",
    "    df = df.drop(columns=[\"statename\", \"districtcode\", \"statecode\", \"districtname\", \"year\", \"crop\"])\n",
    "\n",
    "    # Define columns\n",
    "    log_features = [\n",
    "        \"area1000hectares\",\n",
    "        \"irrigatedarea1000hectares\",\n",
    "        \"unirrigatedarea1000hecatres\",\n",
    "        \"nitrogenconsumptiontonnes\",\n",
    "        \"phosphateconsumptiontonnes\",\n",
    "        \"potashconsumptiontonnes\"\n",
    "    ]\n",
    "    environmental_features = [\"total_rainfall\", \"average_rainfall\", \"salinity_alkalinity_percent\"]\n",
    "\n",
    "    # Replace 0 or negative values with small constant before log\n",
    "    for col in log_features:\n",
    "        df[col] = df[col].clip(lower=1e-6)\n",
    "        df[f\"log_{col}\"] = np.log(df[col])\n",
    "\n",
    "    # Log transform output\n",
    "    df[\"production1000tonnes\"] = df[\"production1000tonnes\"].clip(lower=1e-6)\n",
    "    df[\"log_production\"] = np.log(df[\"production1000tonnes\"])\n",
    "\n",
    "    # One-hot encode regions (drop one category for baseline)\n",
    "    region_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n",
    "    df = df.join(region_dummies)\n",
    "\n",
    "    # Add interaction terms\n",
    "    for var in log_features:\n",
    "        for region in region_dummies.columns:\n",
    "            df[f\"log_{var}_x_{region}\"] = df[f\"log_{var}\"] * df[region]\n",
    "\n",
    "    # Final feature set\n",
    "    feature_cols = (\n",
    "        [f\"log_{col}\" for col in log_features] +\n",
    "        environmental_features +\n",
    "        region_dummies.columns.tolist() +\n",
    "        [f\"log_{var}_x_{region}\" for var in log_features for region in region_dummies.columns]\n",
    "    )\n",
    "\n",
    "    # Make sure all columns are float32\n",
    "    X = df[feature_cols].astype(np.float32)\n",
    "    Y = df[\"log_production\"].astype(np.float32)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "path_to_dataset = \"Task2Summary.csv\"\n",
    "df = pd.read_csv(path_to_dataset)\n",
    "\n",
    "X_df1, Y_df = preprocess_data(df)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_d, Y = torch.tensor(X_df1.to_numpy(), dtype=torch.float32), torch.tensor(Y_df.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "model_a_enhanced = OLS() \n",
    "model_a_enhanced.fit((X_d, Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c419919a",
   "metadata": {},
   "source": [
    "Model-B(Enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08e3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             OLS Regression Results                             \n",
      "================================================================================\n",
      "Dep. Variable:     production1000tonnes   R-squared:                       0.877\n",
      "Model:                              OLS   Adj. R-squared:                  0.867\n",
      "Method:                   Least Squares   F-statistic:                     95.22\n",
      "Date:                  Tue, 22 Apr 2025   Prob (F-statistic):           2.55e-93\n",
      "Time:                          13:02:56   Log-Likelihood:                -885.42\n",
      "No. Observations:                   246   AIC:                             1807.\n",
      "Df Residuals:                       228   BIC:                             1870.\n",
      "Df Model:                            17                                         \n",
      "Covariance Type:              nonrobust                                         \n",
      "=======================================================================================================\n",
      "                                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "const                                 -13.9139      3.888     -3.578      0.000     -21.576      -6.252\n",
      "area1000hectares                        0.6370      0.261      2.440      0.015       0.123       1.151\n",
      "irrigatedarea1000hectares               0.6360      0.523      1.215      0.225      -0.395       1.667\n",
      "nitrogenconsumptiontonnes              -0.0001   9.63e-05     -1.053      0.293      -0.000    8.83e-05\n",
      "phosphateconsumptiontonnes              0.0005      0.000      2.158      0.032    4.62e-05       0.001\n",
      "potashconsumptiontonnes                -0.0007      0.000     -1.994      0.047      -0.001   -8.62e-06\n",
      "total_rainfall                         -0.0150      0.024     -0.623      0.534      -0.063       0.033\n",
      "average_rainfall                        0.6076      0.358      1.696      0.091      -0.098       1.313\n",
      "salinity_alkalinity_percent            -0.1314      0.182     -0.721      0.472      -0.491       0.228\n",
      "unirrigatedarea1000hecatres             0.0009      0.265      0.004      0.997      -0.522       0.524\n",
      "area1000hectares_squared               -0.0014      0.002     -0.661      0.509      -0.005       0.003\n",
      "irrigatedarea1000hectares_squared       0.0099      0.078      0.126      0.900      -0.145       0.164\n",
      "nitrogenconsumptiontonnes_squared    1.207e-09   5.08e-10      2.375      0.018    2.06e-10    2.21e-09\n",
      "phosphateconsumptiontonnes_squared  -1.037e-08   3.29e-09     -3.152      0.002   -1.69e-08   -3.89e-09\n",
      "potashconsumptiontonnes_squared      1.671e-08   1.35e-08      1.236      0.218   -9.92e-09    4.33e-08\n",
      "total_rainfall_squared               1.127e-05   1.74e-05      0.648      0.518    -2.3e-05    4.55e-05\n",
      "average_rainfall_squared               -0.0042      0.003     -1.420      0.157      -0.010       0.002\n",
      "salinity_alkalinity_percent_squared   5.85e-05      0.005      0.013      0.990      -0.009       0.009\n",
      "unirrigatedarea1000hecatres_squared     0.0003      0.002      0.146      0.884      -0.004       0.005\n",
      "==============================================================================\n",
      "Omnibus:                       35.153   Durbin-Watson:                   1.702\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              161.139\n",
      "Skew:                          -0.411   Prob(JB):                     1.02e-35\n",
      "Kurtosis:                       6.879   Cond. No.                     1.21e+18\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is  1e-14. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Task2Summary.csv\")\n",
    "df = df.copy()\n",
    "\n",
    "# Add unirrigated area\n",
    "df[\"unirrigatedarea1000hecatres\"] = df[\"area1000hectares\"] - df[\"irrigatedarea1000hectares\"]\n",
    "\n",
    "# Add region classification\n",
    "region_map = {\n",
    "    \"Chandigarh\": \"North\", \"Delhi\": \"North\", \"Haryana\": \"North\", \"Himachal Pradesh\": \"North\",\n",
    "    \"Jammu & Kashmir\": \"North\", \"Punjab\": \"North\", \"Rajasthan\": \"North\",\n",
    "    \"Arunachal Pradesh\": \"North-East\", \"Assam\": \"North-East\", \"Manipur\": \"North-East\",\n",
    "    \"Meghalaya\": \"North-East\", \"Mizoram\": \"North-East\", \"Nagaland\": \"North-East\", \"Tripura\": \"North-East\",\n",
    "    \"Andaman & Nicobar Islands\": \"East\", \"Bihar\": \"East\", \"Jharkhand\": \"East\",\n",
    "    \"Odisha\": \"East\", \"Sikkim\": \"East\", \"West Bengal\": \"East\",\n",
    "    \"Chhattisgarh\": \"Central\", \"Madhya Pradesh\": \"Central\", \"Uttar Pradesh\": \"Central\", \"Uttarakhand\": \"Central\",\n",
    "    \"Dadra & Nagar Haveli\": \"West\", \"Daman & Diu\": \"West\", \"Goa\": \"West\",\n",
    "    \"Gujarat\": \"West\", \"Maharashtra\": \"West\",\n",
    "    \"Andhra Pradesh\": \"South\", \"Karnataka\": \"South\", \"Kerala\": \"South\",\n",
    "    \"Lakshadweep\": \"South\", \"Puducherry\": \"South\", \"Tamil Nadu\": \"South\"\n",
    "}\n",
    "df[\"region\"] = df[\"statename\"].map(region_map)\n",
    "df = df.dropna(subset=[\"region\"])  # Drop rows where region is not assigned\n",
    "\n",
    "# Base features\n",
    "features = [\n",
    "    \"area1000hectares\", \"irrigatedarea1000hectares\", \"nitrogenconsumptiontonnes\",\n",
    "    \"phosphateconsumptiontonnes\", \"potashconsumptiontonnes\",\n",
    "    \"total_rainfall\", \"average_rainfall\", \"salinity_alkalinity_percent\",\n",
    "    \"unirrigatedarea1000hecatres\"\n",
    "]\n",
    "\n",
    "# Add squared terms\n",
    "for f in features:\n",
    "    df[f\"{f}_squared\"] = df[f] ** 2\n",
    "\n",
    "# Add region dummies (drop one region for baseline)\n",
    "region_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n",
    "df = df.join(region_dummies)\n",
    "\n",
    "# Create interaction terms between region and each feature and squared term\n",
    "for f in features + [f\"{f}_squared\" for f in features]:\n",
    "    for region in region_dummies.columns:\n",
    "        df[f\"{f}_x_{region}\"] = df[f] * df[region]\n",
    "\n",
    "# Define target variable\n",
    "Y = df[\"production1000tonnes\"]\n",
    "'''\n",
    "# Model B (base quadratic model without region interaction)\n",
    "X_base = df[\n",
    "    features + \n",
    "    [f\"{f}_squared\" for f in features]\n",
    "]\n",
    "X_base = sm.add_constant(X_base)\n",
    "model_base = sm.OLS(Y, X_base).fit()\n",
    "'''\n",
    "# Enhanced Model B (with region interaction terms)\n",
    "interaction_cols = [col for col in df.columns if \"_x_\" in col]\n",
    "X_enhanced = df[\n",
    "    features + \n",
    "    [f\"{f}_squared\" for f in features] +\n",
    "    interaction_cols\n",
    "]\n",
    "X_enhanced = sm.add_constant(X_enhanced)\n",
    "model_enhanced = sm.OLS(Y, X_enhanced).fit()\n",
    "\n",
    "# Quadratic Model (refit and print summary of base model)\n",
    "X_quad = df[features + [f\"{f}_squared\" for f in features]]\n",
    "X_quad = sm.add_constant(X_quad)\n",
    "\n",
    "# Ensure index alignment\n",
    "X_quad, Y = X_quad.align(Y, join='inner', axis=0)\n",
    "\n",
    "model_quad = sm.OLS(Y, X_quad).fit()\n",
    "print(model_quad.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feffaeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-test result (F-statistic, p-value, degrees of freedom): (np.float64(2.1984074035452514), np.float64(3.059410660341792e-05), np.float64(66.0))\n"
     ]
    }
   ],
   "source": [
    "# F-test to compare Part B Models\n",
    "f_test_result = model_enhanced.compare_f_test(model_quad)\n",
    "print(\"F-test result (F-statistic, p-value, degrees of freedom):\", f_test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1a1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: -6.0286\n",
      "P-value: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "# Make sure Y is a PyTorch tensor\n",
    "Y_tensor = torch.tensor(Y.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# SSR for enhanced model\n",
    "SSR_e = torch.sum((Y_tensor - model_a_enhanced(X_d)) ** 2).item()\n",
    "k_e = model_a_enhanced.weights.shape[0]  # enhanced model parameters (including bias)\n",
    "df_e = n - k_e  # degrees of freedom for the enhanced model\n",
    "# Calculate the F-statistic\n",
    "F_stat = ((SSR_r - SSR_e) / (k_e - k_r)) / (SSR_e / df_e)\n",
    "p_value = 1 - stats.f.cdf(F_stat, dfn=(k_e - k_r), dfd=(n - k_e))\n",
    "#p_value = 1 - stats.f.cdf(F_stat, k_e - k_r, df_e)\n",
    "\n",
    "print(f\"F-statistic: {F_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "katana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
